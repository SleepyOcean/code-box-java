// é¡¹ç›®ç»“æ„è¯´æ˜ï¼š
// com.sync.MainApp.java                   // é¡¹ç›®å…¥å£
// com.sync.core.SyncManager.java         // å¤šçº¿ç¨‹ä»»åŠ¡ç®¡ç†å™¨
// com.sync.web.SyncServer.java           // æ§åˆ¶æ¥å£æœåŠ¡
// com.sync.kafka.KafkaConsumerToFile.java // ä»é›†ç¾¤Aè¯»å–æ•°æ®å¹¶å†™å…¥æ–‡ä»¶
// com.sync.kafka.KafkaProducerFromFile.java // ä»æ–‡ä»¶è¯»å–æ•°æ®å†™å…¥é›†ç¾¤B
// com.sync.io.AvroFileWriter.java, AvroFileReader.java // Avroæ–‡ä»¶æ“ä½œ
// com.sync.util.ConfigLoader.java        // é…ç½®æ–‡ä»¶åŠ è½½å™¨
// resources/schema/                      // æ¯ä¸ªtopicçš„Avro schemaå®šä¹‰
// config/application.yaml                // é…ç½®æ–‡ä»¶ï¼ˆé›†ç¾¤Aã€Bä¿¡æ¯ã€topicåˆ—è¡¨ç­‰ï¼‰

// com.sync.MainApp.java
package com.sync;

import com.sync.core.SyncManager;
import com.sync.util.ConfigLoader;
import com.sync.web.SyncServer;

public class MainApp {
    public static void main(String[] args) throws Exception {
        Thread.setDefaultUncaughtExceptionHandler((t, e) -> {
            System.err.println("[Uncaught Exception] in thread " + t.getName());
            e.printStackTrace();
        });

        var config = ConfigLoader.load("config/application.yaml");
        SyncManager manager = new SyncManager(config);

        // å¯åŠ¨ REST æ¥å£æœåŠ¡
        SyncServer server = new SyncServer(manager);
        server.start();

        // å¯åŠ¨é»˜è®¤åŒæ­¥ä»»åŠ¡
        manager.startAll();
    }
}

// com.sync.core.SyncManager.java
package com.sync.core;

import com.sync.kafka.KafkaConsumerToFile;
import com.sync.kafka.KafkaProducerFromFile;
import java.util.concurrent.*;
import java.util.*;

public class SyncManager {
    private final Map<String, Object> config;
    private final Map<String, Future<?>> tasks = new ConcurrentHashMap<>();
    private final ExecutorService executor = Executors.newCachedThreadPool();

    public SyncManager(Map<String, Object> config) {
        this.config = config;
    }

    public void startAll() {
        var topics = (Map<String, Object>) config.get("topics");
        for (String topic : topics.keySet()) {
            startTopic(topic);
        }
    }

    public void startTopic(String topic) {
        if (tasks.containsKey(topic)) return;
        Future<?> futureA = executor.submit(new KafkaConsumerToFile(topic, config));
        Future<?> futureB = executor.submit(new KafkaProducerFromFile(topic, config));
        tasks.put(topic + "-A", futureA);
        tasks.put(topic + "-B", futureB);
    }

    public void stopTopic(String topic) {
        Future<?> fA = tasks.remove(topic + "-A");
        Future<?> fB = tasks.remove(topic + "-B");
        if (fA != null) fA.cancel(true);
        if (fB != null) fB.cancel(true);
    }

    public Set<String> getActiveTopics() {
        Set<String> active = new HashSet<>();
        for (String key : tasks.keySet()) {
            active.add(key.split("-")[0]);
        }
        return active;
    }
}

// com.sync.web.SyncServer.java
package com.sync.web;

import com.sun.net.httpserver.*;
import com.sync.core.SyncManager;

import java.io.IOException;
import java.io.OutputStream;
import java.net.InetSocketAddress;

public class SyncServer {
    private final SyncManager manager;

    public SyncServer(SyncManager manager) {
        this.manager = manager;
    }

    public void start() throws IOException {
        HttpServer server = HttpServer.create(new InetSocketAddress(8080), 0);

        server.createContext("/start", exchange -> {
            String topic = exchange.getRequestURI().getQuery().replace("topic=", "");
            manager.startTopic(topic);
            respond(exchange, "Started topic: " + topic);
        });

        server.createContext("/stop", exchange -> {
            String topic = exchange.getRequestURI().getQuery().replace("topic=", "");
            manager.stopTopic(topic);
            respond(exchange, "Stopped topic: " + topic);
        });

        server.createContext("/status", exchange -> {
            respond(exchange, "Active topics: " + manager.getActiveTopics());
        });

        server.setExecutor(null);
        server.start();
        System.out.println("[SyncServer] listening on http://localhost:8080");
    }

    private void respond(HttpExchange exchange, String response) throws IOException {
        exchange.sendResponseHeaders(200, response.length());
        try (OutputStream os = exchange.getResponseBody()) {
            os.write(response.getBytes());
        }
    }
}

// com.sync.util.ConfigLoader.java
package com.sync.util;

import org.yaml.snakeyaml.Yaml;
import java.io.InputStream;
import java.util.Map;

public class ConfigLoader {
    public static Map<String, Object> load(String path) {
        Yaml yaml = new Yaml();
        try (InputStream in = ConfigLoader.class.getClassLoader().getResourceAsStream(path)) {
            if (in == null) throw new IllegalArgumentException("Config file not found: " + path);
            return yaml.load(in);
        } catch (Exception e) {
            throw new RuntimeException("Failed to load config", e);
        }
    }
}

===============================================================================================================


// ...ä¸Šæ–‡çœç•¥

// com.sync.kafka.KafkaConsumerToFile.java
package com.sync.kafka;

import com.sync.io.AvroFileWriter;
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.serialization.ByteArrayDeserializer;

import java.io.File;
import java.time.Duration;
import java.util.*;

public class KafkaConsumerToFile implements Runnable {
    private final String topic;
    private final Map<String, Object> config;

    public KafkaConsumerToFile(String topic, Map<String, Object> config) {
        this.topic = topic;
        this.config = config;
    }

    @Override
    public void run() {
        Map<String, Object> kafkaConfig = (Map<String, Object>) config.get("clusterA");
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaConfig.get("bootstrap").toString());
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "sync-group-" + topic);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

        try (KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(props);
             AvroFileWriter writer = new AvroFileWriter(topic, config)) {
            consumer.subscribe(List.of(topic));
            while (!Thread.currentThread().isInterrupted()) {
                ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofSeconds(1));
                for (ConsumerRecord<byte[], byte[]> record : records) {
                    writer.append(record.value());
                }
            }
        } catch (Exception e) {
            System.err.println("[KafkaConsumerToFile] Error in topic: " + topic);
            e.printStackTrace();
        }
    }
}

// com.sync.kafka.KafkaProducerFromFile.java
package com.sync.kafka;

import com.sync.io.AvroFileReader;
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.ByteArraySerializer;

import java.io.File;
import java.util.Map;
import java.util.Properties;

public class KafkaProducerFromFile implements Runnable {
    private final String topic;
    private final Map<String, Object> config;

    public KafkaProducerFromFile(String topic, Map<String, Object> config) {
        this.topic = topic;
        this.config = config;
    }

    @Override
    public void run() {
        Map<String, Object> kafkaConfig = (Map<String, Object>) config.get("clusterB");
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaConfig.get("bootstrap").toString());
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());

        try (KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(props);
             AvroFileReader reader = new AvroFileReader(topic, config)) {
            byte[] message;
            while ((message = reader.next()) != null && !Thread.currentThread().isInterrupted()) {
                producer.send(new ProducerRecord<>(topic, message));
            }
        } catch (Exception e) {
            System.err.println("[KafkaProducerFromFile] Error in topic: " + topic);
            e.printStackTrace();
        }
    }
}


===============================================================================================================

// ...ä¸Šæ–‡çœç•¥

// com.sync.io.AvroFileWriter.java
package com.sync.io;

import org.apache.avro.Schema;
import org.apache.avro.file.DataFileWriter;
import org.apache.avro.generic.*;
import org.apache.avro.io.DatumWriter;

import java.io.File;
import java.io.IOException;
import java.util.Map;

public class AvroFileWriter implements AutoCloseable {
    private final DataFileWriter<GenericRecord> writer;
    private final Schema schema;
    private final File file;

    public AvroFileWriter(String topic, Map<String, Object> config) throws IOException {
        String schemaPath = "schema/" + topic + ".avsc";
        schema = new Schema.Parser().parse(AvroFileWriter.class.getClassLoader().getResourceAsStream(schemaPath));
        file = new File("data/" + topic + ".avro");

        DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(schema);
        writer = new DataFileWriter<>(datumWriter);
        writer.create(schema, file);
    }

    public void append(byte[] payload) throws IOException {
        GenericRecord record = new GenericData.Record(schema);
        record.put("payload", ByteBuffer.wrap(payload));
        writer.append(record);
    }

    @Override
    public void close() throws IOException {
        writer.close();
    }
}

// com.sync.io.AvroFileReader.java
package com.sync.io;

import org.apache.avro.Schema;
import org.apache.avro.file.DataFileReader;
import org.apache.avro.generic.*;
import org.apache.avro.io.DatumReader;

import java.io.File;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.Map;

public class AvroFileReader implements AutoCloseable {
    private final DataFileReader<GenericRecord> reader;
    private final Schema schema;

    public AvroFileReader(String topic, Map<String, Object> config) throws IOException {
        String schemaPath = "schema/" + topic + ".avsc";
        schema = new Schema.Parser().parse(AvroFileReader.class.getClassLoader().getResourceAsStream(schemaPath));
        File file = new File("data/" + topic + ".avro");

        DatumReader<GenericRecord> datumReader = new GenericDatumReader<>(schema);
        reader = new DataFileReader<>(file, datumReader);
    }

    public byte[] next() throws IOException {
        if (!reader.hasNext()) return null;
        GenericRecord record = reader.next();
        ByteBuffer buffer = (ByteBuffer) record.get("payload");
        byte[] data = new byte[buffer.remaining()];
        buffer.get(data);
        return data;
    }

    @Override
    public void close() throws IOException {
        reader.close();
    }
}

// resources/schema/example-topic.avsc
{
  "type": "record",
  "name": "SyncRecord",
  "fields": [
    {"name": "payload", "type": "bytes"}
  ]
}

// config/application.yaml
clusterA:
  bootstrap: "localhost:9092"

clusterB:
  bootstrap: "localhost:9093"

topics:
  example-topic: {}


===============================================================================================================

// ...ä¸Šæ–‡çœç•¥

// å¢åŠ  Avro æ–‡ä»¶å‹ç¼©å’Œè½®è½¬
// ä¿®æ”¹ AvroFileWriter æ”¯æŒ Snappy å‹ç¼©ä¸æŒ‰å¤§å°åˆ‡åˆ†æ–‡ä»¶

// com.sync.io.AvroFileWriter.java
package com.sync.io;

import org.apache.avro.Schema;
import org.apache.avro.file.CodecFactory;
import org.apache.avro.file.DataFileWriter;
import org.apache.avro.generic.*;
import org.apache.avro.io.DatumWriter;

import java.io.*;
import java.nio.ByteBuffer;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.Map;

public class AvroFileWriter implements AutoCloseable {
    private final Schema schema;
    private final DatumWriter<GenericRecord> datumWriter;
    private final CodecFactory codec;
    private final String topic;
    private final String dir;
    private final long maxSizeBytes;
    private DataFileWriter<GenericRecord> writer;
    private File currentFile;
    private long writtenBytes;

    public AvroFileWriter(String topic, Map<String, Object> config) throws IOException {
        this.topic = topic;
        this.dir = "data/" + topic;
        new File(dir).mkdirs();

        String schemaPath = "schema/" + topic + ".avsc";
        schema = new Schema.Parser().parse(getClass().getClassLoader().getResourceAsStream(schemaPath));
        datumWriter = new GenericDatumWriter<>(schema);
        codec = CodecFactory.snappyCodec();
        maxSizeBytes = 50 * 1024 * 1024; // æ¯50MBåˆ‡ä¸€ä¸ªæ–‡ä»¶

        openNewWriter();
    }

    private void openNewWriter() throws IOException {
        String timestamp = new SimpleDateFormat("yyyyMMdd_HHmmss").format(new Date());
        currentFile = new File(dir + "/" + topic + "_" + timestamp + ".avro");
        writer = new DataFileWriter<>(datumWriter);
        writer.setCodec(codec);
        writer.create(schema, currentFile);
        writtenBytes = 0;
    }

    public void append(byte[] payload) throws IOException {
        GenericRecord record = new GenericData.Record(schema);
        record.put("payload", ByteBuffer.wrap(payload));
        writer.append(record);
        writtenBytes += payload.length + 16;

        if (writtenBytes > maxSizeBytes) {
            writer.close();
            openNewWriter();
        }
    }

    @Override
    public void close() throws IOException {
        writer.close();
    }
}


===============================================================================================================

// ...ä¸Šæ–‡çœç•¥

// AvroFileReader æ”¯æŒéå†å¤šæ–‡ä»¶å’ŒæŒ‰é¡ºåºè¯»å–
// com.sync.io.AvroFileReader.java
package com.sync.io;

import org.apache.avro.Schema;
import org.apache.avro.file.DataFileReader;
import org.apache.avro.generic.*;
import org.apache.avro.io.DatumReader;

import java.io.*;
import java.nio.ByteBuffer;
import java.util.*;

public class AvroFileReader implements AutoCloseable {
    private final Schema schema;
    private final Queue<File> files;
    private final String topic;
    private DataFileReader<GenericRecord> reader;

    public AvroFileReader(String topic, Map<String, Object> config) throws IOException {
        this.topic = topic;
        this.files = new LinkedList<>();
        String dir = "data/" + topic;
        String schemaPath = "schema/" + topic + ".avsc";
        schema = new Schema.Parser().parse(getClass().getClassLoader().getResourceAsStream(schemaPath));

        File[] avroFiles = new File(dir).listFiles((d, name) -> name.endsWith(".avro"));
        if (avroFiles != null) {
            Arrays.sort(avroFiles);
            files.addAll(Arrays.asList(avroFiles));
        }
        openNextReader();
    }

    private void openNextReader() throws IOException {
        if (reader != null) reader.close();
        File nextFile = files.poll();
        if (nextFile != null) {
            DatumReader<GenericRecord> datumReader = new GenericDatumReader<>(schema);
            reader = new DataFileReader<>(nextFile, datumReader);
        } else {
            reader = null;
        }
    }

    public byte[] next() throws IOException {
        while (reader != null) {
            if (reader.hasNext()) {
                GenericRecord record = reader.next();
                ByteBuffer buffer = (ByteBuffer) record.get("payload");
                byte[] data = new byte[buffer.remaining()];
                buffer.get(data);
                return data;
            } else {
                openNextReader();
            }
        }
        return null;
    }

    @Override
    public void close() throws IOException {
        if (reader != null) reader.close();
    }
}

// æ¶ˆè´¹ offset è®°å½•ï¼ˆå¯æ‰©å±•åˆ° Redisã€æ•°æ®åº“ï¼Œæ­¤å¤„ç®€åŒ–ä¸ºæœ¬åœ°æ–‡ä»¶ï¼‰
// com.sync.offset.OffsetTracker.java
package com.sync.offset;

import java.io.*;
import java.util.concurrent.ConcurrentHashMap;

public class OffsetTracker {
    private static final String FILE_PATH = "offset/offsets.txt";
    private static final ConcurrentHashMap<String, Long> offsets = new ConcurrentHashMap<>();

    public static void save(String topic, long offset) {
        offsets.put(topic, offset);
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(FILE_PATH))) {
            for (var entry : offsets.entrySet()) {
                writer.write(entry.getKey() + "," + entry.getValue());
                writer.newLine();
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    public static long load(String topic) {
        File file = new File(FILE_PATH);
        if (!file.exists()) return 0;
        try (BufferedReader reader = new BufferedReader(new FileReader(file))) {
            String line;
            while ((line = reader.readLine()) != null) {
                String[] parts = line.split(",");
                if (parts.length == 2) {
                    offsets.put(parts[0], Long.parseLong(parts[1]));
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
        return offsets.getOrDefault(topic, 0L);
    }
}

// ç®€åŒ–çš„ Maven é…ç½®
// pom.xml
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>com.sync</groupId>
    <artifactId>kafka-file-sync</artifactId>
    <version>1.0.0</version>
    <dependencies>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>3.7.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.avro</groupId>
            <artifactId>avro</artifactId>
            <version>1.11.3</version>
        </dependency>
        <dependency>
            <groupId>org.yaml</groupId>
            <artifactId>snakeyaml</artifactId>
            <version>2.0</version>
        </dependency>
    </dependencies>
</project>


===============================================================================================================

// ...ä¸Šæ–‡çœç•¥

// æ·»åŠ  REST æ§åˆ¶ä¸å…¨å±€å¼‚å¸¸å¤„ç†

// com.sync.api.SyncController.java
package com.sync.api;

import com.sync.task.SyncManager;
import org.springframework.web.bind.annotation.*;
import java.util.*;

@RestController
@RequestMapping("/sync")
public class SyncController {

    @PostMapping("/start")
    public String start(@RequestParam String topic) {
        SyncManager.getInstance().start(topic);
        return "Sync started for: " + topic;
    }

    @PostMapping("/stop")
    public String stop(@RequestParam String topic) {
        SyncManager.getInstance().stop(topic);
        return "Sync stopped for: " + topic;
    }

    @GetMapping("/status")
    public Map<String, String> status() {
        return SyncManager.getInstance().status();
    }
}

// com.sync.task.SyncManager.java
package com.sync.task;

import java.util.*;
import java.util.concurrent.*;

public class SyncManager {
    private final Map<String, Future<?>> tasks = new ConcurrentHashMap<>();
    private final ExecutorService executor = Executors.newCachedThreadPool();
    private static final SyncManager INSTANCE = new SyncManager();

    public static SyncManager getInstance() {
        return INSTANCE;
    }

    public void start(String topic) {
        if (!tasks.containsKey(topic)) {
            Future<?> future = executor.submit(new SyncRunnable(topic));
            tasks.put(topic, future);
        }
    }

    public void stop(String topic) {
        Future<?> future = tasks.remove(topic);
        if (future != null) future.cancel(true);
    }

    public Map<String, String> status() {
        Map<String, String> result = new HashMap<>();
        for (var entry : tasks.entrySet()) {
            result.put(entry.getKey(), entry.getValue().isCancelled() ? "stopped" : "running");
        }
        return result;
    }
}

// com.sync.task.SyncRunnable.java
package com.sync.task;

public class SyncRunnable implements Runnable {
    private final String topic;

    public SyncRunnable(String topic) {
        this.topic = topic;
    }

    @Override
    public void run() {
        try {
            // å®é™…æ¶ˆè´¹å†™æ–‡ä»¶ã€å†ç”Ÿäº§åˆ°é›†ç¾¤Bçš„ä¸»é€»è¾‘åº”å°è£…ä¸ºæœåŠ¡ç±»åœ¨æ­¤è°ƒç”¨
            System.out.println("[Sync] Running sync task for topic: " + topic);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

// com.sync.GlobalExceptionHandler.java
package com.sync;

import org.springframework.http.*;
import org.springframework.web.bind.annotation.*;

@RestControllerAdvice
public class GlobalExceptionHandler {
    @ExceptionHandler(Exception.class)
    public ResponseEntity<String> handle(Exception e) {
        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body("Error: " + e.getMessage());
    }
}

// SpringBoot å¯åŠ¨ç±»
// com.sync.KafkaSyncApplication.java
package com.sync;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class KafkaSyncApplication {
    public static void main(String[] args) {
        SpringApplication.run(KafkaSyncApplication.class, args);
    }
}

// pom.xml å¢åŠ  Spring Boot ä¾èµ–
<dependency>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter-web</artifactId>
</dependency>
<dependency>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter</artifactId>
</dependency>
<dependency>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>



===============================================================================================================

# Kafka File-Based Sync (Avro)

æœ¬é¡¹ç›®å®ç° Kafka é›†ç¾¤ A åˆ°é›†ç¾¤ B çš„å¼‚æ­¥æ•°æ®åŒæ­¥ã€‚ä¸¤é›†ç¾¤ç½‘ç»œä¸é€šï¼Œé€šè¿‡ä¸­è½¬æ–‡ä»¶å®Œæˆæ•°æ®ä¼ è¾“ï¼Œæ”¯æŒåƒä¸‡çº§æ¶ˆæ¯åŒæ­¥ï¼Œæ ¼å¼ç»Ÿä¸€ä¸º Avroï¼ˆå‹ç¼©ã€å¯è½®è½¬ï¼‰ã€‚

## ğŸ”§ åŠŸèƒ½ç‰¹æ€§

- æ”¯æŒ Avro æ ¼å¼ä¸­è½¬æ–‡ä»¶ï¼ˆSnappy å‹ç¼©ï¼‰
- æ”¯æŒæ•°æ®åŸæ ·ï¼ˆå­—èŠ‚çº§ï¼‰åŒæ­¥
- Kafka å¤š topic æ”¯æŒ
- è‡ªåŠ¨åˆ†å‰²æ–‡ä»¶ï¼ˆé»˜è®¤ 50MBï¼‰
- æœ¬åœ° offset æŒä¹…åŒ–é˜²æ­¢é‡å¤æ¶ˆè´¹
- REST æ¥å£æ§åˆ¶ä»»åŠ¡ï¼ˆå¯åŠ¨/åœæ­¢/æŸ¥è¯¢ï¼‰
- å…¨å±€å¼‚å¸¸å¤„ç†ã€å¯æ‰©å±•æ—¥å¿—ç»Ÿä¸€
- å¤šçº¿ç¨‹ä»»åŠ¡å¤„ç†è°ƒåº¦

## ğŸ“¦ é¡¹ç›®ç»“æ„

```
kafka-file-sync/
â”œâ”€â”€ schema/               # æ¯ä¸ª topic çš„ .avsc æ–‡ä»¶
â”œâ”€â”€ data/                 # Avro ä¸­è½¬æ–‡ä»¶ç›®å½•
â”œâ”€â”€ offset/               # æœ¬åœ° offset æŒä¹…åŒ–
â”œâ”€â”€ src/main/java/
â”‚   â”œâ”€â”€ com.sync.api/         # REST æ§åˆ¶å™¨
â”‚   â”œâ”€â”€ com.sync.io/          # Avro å†™å…¥/è¯»å–æ¨¡å—
â”‚   â”œâ”€â”€ com.sync.offset/      # offset å­˜å‚¨æ¨¡å—
â”‚   â”œâ”€â”€ com.sync.task/        # å¤šçº¿ç¨‹ä»»åŠ¡è°ƒåº¦å™¨
â”‚   â””â”€â”€ com.sync/             # å¯åŠ¨å…¥å£ã€å¼‚å¸¸å¤„ç†
â”œâ”€â”€ src/main/resources/
â”‚   â”œâ”€â”€ application.yaml
â”‚   â””â”€â”€ schema/topic-name.avsc
â”œâ”€â”€ pom.xml
â””â”€â”€ README.md
```

## ğŸš€ å¯åŠ¨æ–¹å¼

```bash
# æ„å»ºé¡¹ç›®
mvn clean package

# è¿è¡Œ Spring Boot æœåŠ¡ï¼ˆåŒ…å« REST æ¥å£ï¼‰
java -jar target/kafka-file-sync-1.0.0.jar
```

## ğŸ§ª REST æ¥å£ä½¿ç”¨ç¤ºä¾‹

```bash
curl -X POST 'http://localhost:8080/sync/start?topic=your-topic'
curl -X POST 'http://localhost:8080/sync/stop?topic=your-topic'
curl http://localhost:8080/sync/status
```

## ğŸ› ï¸ Docker éƒ¨ç½²ï¼ˆå¯é€‰ï¼‰

```Dockerfile
FROM openjdk:17-alpine
WORKDIR /app
COPY target/kafka-file-sync-1.0.0.jar app.jar
EXPOSE 8080
ENTRYPOINT ["java", "-jar", "app.jar"]
```

```bash
# æ„å»ºé•œåƒ
docker build -t kafka-sync .

# å¯åŠ¨å®¹å™¨
docker run -p 8080:8080 -v $(pwd)/data:/app/data kafka-sync
```

## ğŸ“Œ é…ç½®é¡¹ï¼ˆapplication.yamlï¼‰

```yaml
kafka:
  clusterA:
    bootstrapServers: "localhost:9092"
  clusterB:
    bootstrapServers: "localhost:9093"
```

## âœ… ç¤ºä¾‹ Avro Schemaï¼ˆschema/example-topic.avscï¼‰

```json
{
  "type": "record",
  "name": "Message",
  "fields": [
    { "name": "payload", "type": "bytes" }
  ]
}
```

---

ğŸ“¨ å¦‚éœ€æ”¯æŒæ›´å¤šåŠŸèƒ½ï¼ˆSchema Evolutionã€å¤šæ ¼å¼æ··åˆç­‰ï¼‰ï¼Œè¯·æäº¤ Issue æˆ– Fork æœ¬é¡¹ç›®ã€‚

MIT License Â© 2025
