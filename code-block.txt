// È°πÁõÆÁªìÊûÑËØ¥ÊòéÔºö
// com.sync.MainApp.java                   // È°πÁõÆÂÖ•Âè£
// com.sync.core.SyncManager.java         // Â§öÁ∫øÁ®ã‰ªªÂä°ÁÆ°ÁêÜÂô®
// com.sync.web.SyncServer.java           // ÊéßÂà∂Êé•Âè£ÊúçÂä°
// com.sync.kafka.KafkaConsumerToFile.java // ‰ªéÈõÜÁæ§AËØªÂèñÊï∞ÊçÆÂπ∂ÂÜôÂÖ•Êñá‰ª∂
// com.sync.kafka.KafkaProducerFromFile.java // ‰ªéÊñá‰ª∂ËØªÂèñÊï∞ÊçÆÂÜôÂÖ•ÈõÜÁæ§B
// com.sync.io.AvroFileWriter.java, AvroFileReader.java // AvroÊñá‰ª∂Êìç‰Ωú
// com.sync.util.ConfigLoader.java        // ÈÖçÁΩÆÊñá‰ª∂Âä†ËΩΩÂô®
// resources/schema/                      // ÊØè‰∏™topicÁöÑAvro schemaÂÆö‰πâ
// config/application.yaml                // ÈÖçÁΩÆÊñá‰ª∂ÔºàÈõÜÁæ§A„ÄÅB‰ø°ÊÅØ„ÄÅtopicÂàóË°®Á≠âÔºâ

// com.sync.MainApp.java
package com.sync;

import com.sync.core.SyncManager;
import com.sync.util.ConfigLoader;
import com.sync.web.SyncServer;

public class MainApp {
    public static void main(String[] args) throws Exception {
        Thread.setDefaultUncaughtExceptionHandler((t, e) -> {
            System.err.println("[Uncaught Exception] in thread " + t.getName());
            e.printStackTrace();
        });

        var config = ConfigLoader.load("config/application.yaml");
        SyncManager manager = new SyncManager(config);

        // ÂêØÂä® REST Êé•Âè£ÊúçÂä°
        SyncServer server = new SyncServer(manager);
        server.start();

        // ÂêØÂä®ÈªòËÆ§ÂêåÊ≠•‰ªªÂä°
        manager.startAll();
    }
}

// com.sync.core.SyncManager.java
package com.sync.core;

import com.sync.kafka.KafkaConsumerToFile;
import com.sync.kafka.KafkaProducerFromFile;
import java.util.concurrent.*;
import java.util.*;

public class SyncManager {
    private final Map<String, Object> config;
    private final Map<String, Future<?>> tasks = new ConcurrentHashMap<>();
    private final ExecutorService executor = Executors.newCachedThreadPool();

    public SyncManager(Map<String, Object> config) {
        this.config = config;
    }

    public void startAll() {
        var topics = (Map<String, Object>) config.get("topics");
        for (String topic : topics.keySet()) {
            startTopic(topic);
        }
    }

    public void startTopic(String topic) {
        if (tasks.containsKey(topic)) return;
        Future<?> futureA = executor.submit(new KafkaConsumerToFile(topic, config));
        Future<?> futureB = executor.submit(new KafkaProducerFromFile(topic, config));
        tasks.put(topic + "-A", futureA);
        tasks.put(topic + "-B", futureB);
    }

    public void stopTopic(String topic) {
        Future<?> fA = tasks.remove(topic + "-A");
        Future<?> fB = tasks.remove(topic + "-B");
        if (fA != null) fA.cancel(true);
        if (fB != null) fB.cancel(true);
    }

    public Set<String> getActiveTopics() {
        Set<String> active = new HashSet<>();
        for (String key : tasks.keySet()) {
            active.add(key.split("-")[0]);
        }
        return active;
    }
}

// com.sync.web.SyncServer.java
package com.sync.web;

import com.sun.net.httpserver.*;
import com.sync.core.SyncManager;

import java.io.IOException;
import java.io.OutputStream;
import java.net.InetSocketAddress;

public class SyncServer {
    private final SyncManager manager;

    public SyncServer(SyncManager manager) {
        this.manager = manager;
    }

    public void start() throws IOException {
        HttpServer server = HttpServer.create(new InetSocketAddress(8080), 0);

        server.createContext("/start", exchange -> {
            String topic = exchange.getRequestURI().getQuery().replace("topic=", "");
            manager.startTopic(topic);
            respond(exchange, "Started topic: " + topic);
        });

        server.createContext("/stop", exchange -> {
            String topic = exchange.getRequestURI().getQuery().replace("topic=", "");
            manager.stopTopic(topic);
            respond(exchange, "Stopped topic: " + topic);
        });

        server.createContext("/status", exchange -> {
            respond(exchange, "Active topics: " + manager.getActiveTopics());
        });

        server.setExecutor(null);
        server.start();
        System.out.println("[SyncServer] listening on http://localhost:8080");
    }

    private void respond(HttpExchange exchange, String response) throws IOException {
        exchange.sendResponseHeaders(200, response.length());
        try (OutputStream os = exchange.getResponseBody()) {
            os.write(response.getBytes());
        }
    }
}

// com.sync.util.ConfigLoader.java
package com.sync.util;

import org.yaml.snakeyaml.Yaml;
import java.io.InputStream;
import java.util.Map;

public class ConfigLoader {
    public static Map<String, Object> load(String path) {
        Yaml yaml = new Yaml();
        try (InputStream in = ConfigLoader.class.getClassLoader().getResourceAsStream(path)) {
            if (in == null) throw new IllegalArgumentException("Config file not found: " + path);
            return yaml.load(in);
        } catch (Exception e) {
            throw new RuntimeException("Failed to load config", e);
        }
    }
}

===============================================================================================================


// ...‰∏äÊñáÁúÅÁï•

// com.sync.kafka.KafkaConsumerToFile.java
package com.sync.kafka;

import com.sync.io.AvroFileWriter;
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.serialization.ByteArrayDeserializer;

import java.io.File;
import java.time.Duration;
import java.util.*;

public class KafkaConsumerToFile implements Runnable {
    private final String topic;
    private final Map<String, Object> config;

    public KafkaConsumerToFile(String topic, Map<String, Object> config) {
        this.topic = topic;
        this.config = config;
    }

    @Override
    public void run() {
        Map<String, Object> kafkaConfig = (Map<String, Object>) config.get("clusterA");
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaConfig.get("bootstrap").toString());
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "sync-group-" + topic);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

        try (KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(props);
             AvroFileWriter writer = new AvroFileWriter(topic, config)) {
            consumer.subscribe(List.of(topic));
            while (!Thread.currentThread().isInterrupted()) {
                ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofSeconds(1));
                for (ConsumerRecord<byte[], byte[]> record : records) {
                    writer.append(record.value());
                }
            }
        } catch (Exception e) {
            System.err.println("[KafkaConsumerToFile] Error in topic: " + topic);
            e.printStackTrace();
        }
    }
}

// com.sync.kafka.KafkaProducerFromFile.java
package com.sync.kafka;

import com.sync.io.AvroFileReader;
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.ByteArraySerializer;

import java.io.File;
import java.util.Map;
import java.util.Properties;

public class KafkaProducerFromFile implements Runnable {
    private final String topic;
    private final Map<String, Object> config;

    public KafkaProducerFromFile(String topic, Map<String, Object> config) {
        this.topic = topic;
        this.config = config;
    }

    @Override
    public void run() {
        Map<String, Object> kafkaConfig = (Map<String, Object>) config.get("clusterB");
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaConfig.get("bootstrap").toString());
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());

        try (KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(props);
             AvroFileReader reader = new AvroFileReader(topic, config)) {
            byte[] message;
            while ((message = reader.next()) != null && !Thread.currentThread().isInterrupted()) {
                producer.send(new ProducerRecord<>(topic, message));
            }
        } catch (Exception e) {
            System.err.println("[KafkaProducerFromFile] Error in topic: " + topic);
            e.printStackTrace();
        }
    }
}


===============================================================================================================

// ...‰∏äÊñáÁúÅÁï•

// com.sync.io.AvroFileWriter.java
package com.sync.io;

import org.apache.avro.Schema;
import org.apache.avro.file.DataFileWriter;
import org.apache.avro.generic.*;
import org.apache.avro.io.DatumWriter;

import java.io.File;
import java.io.IOException;
import java.util.Map;

public class AvroFileWriter implements AutoCloseable {
    private final DataFileWriter<GenericRecord> writer;
    private final Schema schema;
    private final File file;

    public AvroFileWriter(String topic, Map<String, Object> config) throws IOException {
        String schemaPath = "schema/" + topic + ".avsc";
        schema = new Schema.Parser().parse(AvroFileWriter.class.getClassLoader().getResourceAsStream(schemaPath));
        file = new File("data/" + topic + ".avro");

        DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(schema);
        writer = new DataFileWriter<>(datumWriter);
        writer.create(schema, file);
    }

    public void append(byte[] payload) throws IOException {
        GenericRecord record = new GenericData.Record(schema);
        record.put("payload", ByteBuffer.wrap(payload));
        writer.append(record);
    }

    @Override
    public void close() throws IOException {
        writer.close();
    }
}

// com.sync.io.AvroFileReader.java
package com.sync.io;

import org.apache.avro.Schema;
import org.apache.avro.file.DataFileReader;
import org.apache.avro.generic.*;
import org.apache.avro.io.DatumReader;

import java.io.File;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.Map;

public class AvroFileReader implements AutoCloseable {
    private final DataFileReader<GenericRecord> reader;
    private final Schema schema;

    public AvroFileReader(String topic, Map<String, Object> config) throws IOException {
        String schemaPath = "schema/" + topic + ".avsc";
        schema = new Schema.Parser().parse(AvroFileReader.class.getClassLoader().getResourceAsStream(schemaPath));
        File file = new File("data/" + topic + ".avro");

        DatumReader<GenericRecord> datumReader = new GenericDatumReader<>(schema);
        reader = new DataFileReader<>(file, datumReader);
    }

    public byte[] next() throws IOException {
        if (!reader.hasNext()) return null;
        GenericRecord record = reader.next();
        ByteBuffer buffer = (ByteBuffer) record.get("payload");
        byte[] data = new byte[buffer.remaining()];
        buffer.get(data);
        return data;
    }

    @Override
    public void close() throws IOException {
        reader.close();
    }
}

// resources/schema/example-topic.avsc
{
  "type": "record",
  "name": "SyncRecord",
  "fields": [
    {"name": "payload", "type": "bytes"}
  ]
}

// config/application.yaml
clusterA:
  bootstrap: "localhost:9092"

clusterB:
  bootstrap: "localhost:9093"

topics:
  example-topic: {}


===============================================================================================================

// ...‰∏äÊñáÁúÅÁï•

// Â¢ûÂä† Avro Êñá‰ª∂ÂéãÁº©ÂíåËΩÆËΩ¨
// ‰øÆÊîπ AvroFileWriter ÊîØÊåÅ Snappy ÂéãÁº©‰∏éÊåâÂ§ßÂ∞èÂàáÂàÜÊñá‰ª∂

// com.sync.io.AvroFileWriter.java
package com.sync.io;

import org.apache.avro.Schema;
import org.apache.avro.file.CodecFactory;
import org.apache.avro.file.DataFileWriter;
import org.apache.avro.generic.*;
import org.apache.avro.io.DatumWriter;

import java.io.*;
import java.nio.ByteBuffer;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.Map;

public class AvroFileWriter implements AutoCloseable {
    private final Schema schema;
    private final DatumWriter<GenericRecord> datumWriter;
    private final CodecFactory codec;
    private final String topic;
    private final String dir;
    private final long maxSizeBytes;
    private DataFileWriter<GenericRecord> writer;
    private File currentFile;
    private long writtenBytes;

    public AvroFileWriter(String topic, Map<String, Object> config) throws IOException {
        this.topic = topic;
        this.dir = "data/" + topic;
        new File(dir).mkdirs();

        String schemaPath = "schema/" + topic + ".avsc";
        schema = new Schema.Parser().parse(getClass().getClassLoader().getResourceAsStream(schemaPath));
        datumWriter = new GenericDatumWriter<>(schema);
        codec = CodecFactory.snappyCodec();
        maxSizeBytes = 50 * 1024 * 1024; // ÊØè50MBÂàá‰∏Ä‰∏™Êñá‰ª∂

        openNewWriter();
    }

    private void openNewWriter() throws IOException {
        String timestamp = new SimpleDateFormat("yyyyMMdd_HHmmss").format(new Date());
        currentFile = new File(dir + "/" + topic + "_" + timestamp + ".avro");
        writer = new DataFileWriter<>(datumWriter);
        writer.setCodec(codec);
        writer.create(schema, currentFile);
        writtenBytes = 0;
    }

    public void append(byte[] payload) throws IOException {
        GenericRecord record = new GenericData.Record(schema);
        record.put("payload", ByteBuffer.wrap(payload));
        writer.append(record);
        writtenBytes += payload.length + 16;

        if (writtenBytes > maxSizeBytes) {
            writer.close();
            openNewWriter();
        }
    }

    @Override
    public void close() throws IOException {
        writer.close();
    }
}


===============================================================================================================

// ...‰∏äÊñáÁúÅÁï•

// AvroFileReader ÊîØÊåÅÈÅçÂéÜÂ§öÊñá‰ª∂ÂíåÊåâÈ°∫Â∫èËØªÂèñ
// com.sync.io.AvroFileReader.java
package com.sync.io;

import org.apache.avro.Schema;
import org.apache.avro.file.DataFileReader;
import org.apache.avro.generic.*;
import org.apache.avro.io.DatumReader;

import java.io.*;
import java.nio.ByteBuffer;
import java.util.*;

public class AvroFileReader implements AutoCloseable {
    private final Schema schema;
    private final Queue<File> files;
    private final String topic;
    private DataFileReader<GenericRecord> reader;

    public AvroFileReader(String topic, Map<String, Object> config) throws IOException {
        this.topic = topic;
        this.files = new LinkedList<>();
        String dir = "data/" + topic;
        String schemaPath = "schema/" + topic + ".avsc";
        schema = new Schema.Parser().parse(getClass().getClassLoader().getResourceAsStream(schemaPath));

        File[] avroFiles = new File(dir).listFiles((d, name) -> name.endsWith(".avro"));
        if (avroFiles != null) {
            Arrays.sort(avroFiles);
            files.addAll(Arrays.asList(avroFiles));
        }
        openNextReader();
    }

    private void openNextReader() throws IOException {
        if (reader != null) reader.close();
        File nextFile = files.poll();
        if (nextFile != null) {
            DatumReader<GenericRecord> datumReader = new GenericDatumReader<>(schema);
            reader = new DataFileReader<>(nextFile, datumReader);
        } else {
            reader = null;
        }
    }

    public byte[] next() throws IOException {
        while (reader != null) {
            if (reader.hasNext()) {
                GenericRecord record = reader.next();
                ByteBuffer buffer = (ByteBuffer) record.get("payload");
                byte[] data = new byte[buffer.remaining()];
                buffer.get(data);
                return data;
            } else {
                openNextReader();
            }
        }
        return null;
    }

    @Override
    public void close() throws IOException {
        if (reader != null) reader.close();
    }
}

// Ê∂àË¥π offset ËÆ∞ÂΩïÔºàÂèØÊâ©Â±ïÂà∞ Redis„ÄÅÊï∞ÊçÆÂ∫ìÔºåÊ≠§Â§ÑÁÆÄÂåñ‰∏∫Êú¨Âú∞Êñá‰ª∂Ôºâ
// com.sync.offset.OffsetTracker.java
package com.sync.offset;

import java.io.*;
import java.util.concurrent.ConcurrentHashMap;

public class OffsetTracker {
    private static final String FILE_PATH = "offset/offsets.txt";
    private static final ConcurrentHashMap<String, Long> offsets = new ConcurrentHashMap<>();

    public static void save(String topic, long offset) {
        offsets.put(topic, offset);
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(FILE_PATH))) {
            for (var entry : offsets.entrySet()) {
                writer.write(entry.getKey() + "," + entry.getValue());
                writer.newLine();
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    public static long load(String topic) {
        File file = new File(FILE_PATH);
        if (!file.exists()) return 0;
        try (BufferedReader reader = new BufferedReader(new FileReader(file))) {
            String line;
            while ((line = reader.readLine()) != null) {
                String[] parts = line.split(",");
                if (parts.length == 2) {
                    offsets.put(parts[0], Long.parseLong(parts[1]));
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
        return offsets.getOrDefault(topic, 0L);
    }
}

// ÁÆÄÂåñÁöÑ Maven ÈÖçÁΩÆ
// pom.xml
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>com.sync</groupId>
    <artifactId>kafka-file-sync</artifactId>
    <version>1.0.0</version>
    <dependencies>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>3.7.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.avro</groupId>
            <artifactId>avro</artifactId>
            <version>1.11.3</version>
        </dependency>
        <dependency>
            <groupId>org.yaml</groupId>
            <artifactId>snakeyaml</artifactId>
            <version>2.0</version>
        </dependency>
    </dependencies>
</project>


===============================================================================================================

// ...‰∏äÊñáÁúÅÁï•

// Ê∑ªÂä† REST ÊéßÂà∂‰∏éÂÖ®Â±ÄÂºÇÂ∏∏Â§ÑÁêÜ

// com.sync.api.SyncController.java
package com.sync.api;

import com.sync.task.SyncManager;
import org.springframework.web.bind.annotation.*;
import java.util.*;

@RestController
@RequestMapping("/sync")
public class SyncController {

    @PostMapping("/start")
    public String start(@RequestParam String topic) {
        SyncManager.getInstance().start(topic);
        return "Sync started for: " + topic;
    }

    @PostMapping("/stop")
    public String stop(@RequestParam String topic) {
        SyncManager.getInstance().stop(topic);
        return "Sync stopped for: " + topic;
    }

    @GetMapping("/status")
    public Map<String, String> status() {
        return SyncManager.getInstance().status();
    }
}

// com.sync.task.SyncManager.java
package com.sync.task;

import java.util.*;
import java.util.concurrent.*;

public class SyncManager {
    private final Map<String, Future<?>> tasks = new ConcurrentHashMap<>();
    private final ExecutorService executor = Executors.newCachedThreadPool();
    private static final SyncManager INSTANCE = new SyncManager();

    public static SyncManager getInstance() {
        return INSTANCE;
    }

    public void start(String topic) {
        if (!tasks.containsKey(topic)) {
            Future<?> future = executor.submit(new SyncRunnable(topic));
            tasks.put(topic, future);
        }
    }

    public void stop(String topic) {
        Future<?> future = tasks.remove(topic);
        if (future != null) future.cancel(true);
    }

    public Map<String, String> status() {
        Map<String, String> result = new HashMap<>();
        for (var entry : tasks.entrySet()) {
            result.put(entry.getKey(), entry.getValue().isCancelled() ? "stopped" : "running");
        }
        return result;
    }
}

// com.sync.task.SyncRunnable.java
package com.sync.task;

public class SyncRunnable implements Runnable {
    private final String topic;

    public SyncRunnable(String topic) {
        this.topic = topic;
    }

    @Override
    public void run() {
        try {
            // ÂÆûÈôÖÊ∂àË¥πÂÜôÊñá‰ª∂„ÄÅÂÜçÁîü‰∫ßÂà∞ÈõÜÁæ§BÁöÑ‰∏ªÈÄªËæëÂ∫îÂ∞ÅË£Ö‰∏∫ÊúçÂä°Á±ªÂú®Ê≠§Ë∞ÉÁî®
            System.out.println("[Sync] Running sync task for topic: " + topic);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

// com.sync.GlobalExceptionHandler.java
package com.sync;

import org.springframework.http.*;
import org.springframework.web.bind.annotation.*;

@RestControllerAdvice
public class GlobalExceptionHandler {
    @ExceptionHandler(Exception.class)
    public ResponseEntity<String> handle(Exception e) {
        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body("Error: " + e.getMessage());
    }
}

// SpringBoot ÂêØÂä®Á±ª
// com.sync.KafkaSyncApplication.java
package com.sync;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class KafkaSyncApplication {
    public static void main(String[] args) {
        SpringApplication.run(KafkaSyncApplication.class, args);
    }
}

// pom.xml Â¢ûÂä† Spring Boot ‰æùËµñ
<dependency>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter-web</artifactId>
</dependency>
<dependency>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter</artifactId>
</dependency>
<dependency>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>



===============================================================================================================

# Kafka File-Based Sync (Avro)

Êú¨È°πÁõÆÂÆûÁé∞ Kafka ÈõÜÁæ§ A Âà∞ÈõÜÁæ§ B ÁöÑÂºÇÊ≠•Êï∞ÊçÆÂêåÊ≠•„ÄÇ‰∏§ÈõÜÁæ§ÁΩëÁªú‰∏çÈÄöÔºåÈÄöËøá‰∏≠ËΩ¨Êñá‰ª∂ÂÆåÊàêÊï∞ÊçÆ‰º†ËæìÔºåÊîØÊåÅÂçÉ‰∏áÁ∫ßÊ∂àÊÅØÂêåÊ≠•ÔºåÊ†ºÂºèÁªü‰∏Ä‰∏∫ AvroÔºàÂéãÁº©„ÄÅÂèØËΩÆËΩ¨Ôºâ„ÄÇ

## üîß ÂäüËÉΩÁâπÊÄß

- ÊîØÊåÅ Avro Ê†ºÂºè‰∏≠ËΩ¨Êñá‰ª∂ÔºàSnappy ÂéãÁº©Ôºâ
- ÊîØÊåÅÊï∞ÊçÆÂéüÊ†∑ÔºàÂ≠óËäÇÁ∫ßÔºâÂêåÊ≠•
- Kafka Â§ö topic ÊîØÊåÅ
- Ëá™Âä®ÂàÜÂâ≤Êñá‰ª∂ÔºàÈªòËÆ§ 50MBÔºâ
- Êú¨Âú∞ offset ÊåÅ‰πÖÂåñÈò≤Ê≠¢ÈáçÂ§çÊ∂àË¥π
- REST Êé•Âè£ÊéßÂà∂‰ªªÂä°ÔºàÂêØÂä®/ÂÅúÊ≠¢/Êü•ËØ¢Ôºâ
- ÂÖ®Â±ÄÂºÇÂ∏∏Â§ÑÁêÜ„ÄÅÂèØÊâ©Â±ïÊó•ÂøóÁªü‰∏Ä
- Â§öÁ∫øÁ®ã‰ªªÂä°Â§ÑÁêÜË∞ÉÂ∫¶

## üì¶ È°πÁõÆÁªìÊûÑ

```
kafka-file-sync/
‚îú‚îÄ‚îÄ schema/               # ÊØè‰∏™ topic ÁöÑ .avsc Êñá‰ª∂
‚îú‚îÄ‚îÄ data/                 # Avro ‰∏≠ËΩ¨Êñá‰ª∂ÁõÆÂΩï
‚îú‚îÄ‚îÄ offset/               # Êú¨Âú∞ offset ÊåÅ‰πÖÂåñ
‚îú‚îÄ‚îÄ src/main/java/
‚îÇ   ‚îú‚îÄ‚îÄ com.sync.api/         # REST ÊéßÂà∂Âô®
‚îÇ   ‚îú‚îÄ‚îÄ com.sync.io/          # Avro ÂÜôÂÖ•/ËØªÂèñÊ®°Âùó
‚îÇ   ‚îú‚îÄ‚îÄ com.sync.offset/      # offset Â≠òÂÇ®Ê®°Âùó
‚îÇ   ‚îú‚îÄ‚îÄ com.sync.task/        # Â§öÁ∫øÁ®ã‰ªªÂä°Ë∞ÉÂ∫¶Âô®
‚îÇ   ‚îî‚îÄ‚îÄ com.sync/             # ÂêØÂä®ÂÖ•Âè£„ÄÅÂºÇÂ∏∏Â§ÑÁêÜ
‚îú‚îÄ‚îÄ src/main/resources/
‚îÇ   ‚îú‚îÄ‚îÄ application.yaml
‚îÇ   ‚îî‚îÄ‚îÄ schema/topic-name.avsc
‚îú‚îÄ‚îÄ pom.xml
‚îî‚îÄ‚îÄ README.md
```

## üöÄ ÂêØÂä®ÊñπÂºè

```bash
# ÊûÑÂª∫È°πÁõÆ
mvn clean package

# ËøêË°å Spring Boot ÊúçÂä°ÔºàÂåÖÂê´ REST Êé•Âè£Ôºâ
java -jar target/kafka-file-sync-1.0.0.jar
```

## üß™ REST Êé•Âè£‰ΩøÁî®Á§∫‰æã

```bash
curl -X POST 'http://localhost:8080/sync/start?topic=your-topic'
curl -X POST 'http://localhost:8080/sync/stop?topic=your-topic'
curl http://localhost:8080/sync/status
```

## üõ†Ô∏è Docker ÈÉ®ÁΩ≤ÔºàÂèØÈÄâÔºâ

```Dockerfile
FROM openjdk:17-alpine
WORKDIR /app
COPY target/kafka-file-sync-1.0.0.jar app.jar
EXPOSE 8080
ENTRYPOINT ["java", "-jar", "app.jar"]
```

```bash
# ÊûÑÂª∫ÈïúÂÉè
docker build -t kafka-sync .

# ÂêØÂä®ÂÆπÂô®
docker run -p 8080:8080 -v $(pwd)/data:/app/data kafka-sync
```

## üìå ÈÖçÁΩÆÈ°πÔºàapplication.yamlÔºâ

```yaml
kafka:
  clusterA:
    bootstrapServers: "localhost:9092"
  clusterB:
    bootstrapServers: "localhost:9093"
```

## ‚úÖ Á§∫‰æã Avro SchemaÔºàschema/example-topic.avscÔºâ

```json
{
  "type": "record",
  "name": "Message",
  "fields": [
    { "name": "payload", "type": "bytes" }
  ]
}
```

---

üì® Â¶ÇÈúÄÊîØÊåÅÊõ¥Â§öÂäüËÉΩÔºàSchema Evolution„ÄÅÂ§öÊ†ºÂºèÊ∑∑ÂêàÁ≠âÔºâÔºåËØ∑Êèê‰∫§ Issue Êàñ Fork Êú¨È°πÁõÆ„ÄÇ

MIT License ¬© 2025


-----------


#!/bin/bash
# Á∫Ø Bash ÂÆûÁé∞ÔºöÊØèÂ§© 11„ÄÅ13„ÄÅ15„ÄÅ16 ÁÇπÊï¥ÁÇπËß¶ÂèëÊã∑Ë¥ù
# ‰∏ç‰æùËµñ crontab„ÄÅsystemd„ÄÅat Á≠â‰ªª‰ΩïÂ§ñÈÉ®ÂÆöÊó∂Â∑•ÂÖ∑

########### Áî®Êà∑ÂèØ‰øÆÊîπÁöÑÈÉ®ÂàÜ ###########
SRC_DIR="/source/dir"          # Ê∫êÁõÆÂΩï
DEST_BASE="/backup/dir"        # Â§á‰ªΩÊ†πÁõÆÂΩï
LOG_FILE="/tmp/pure_copy.log"  # Êó•Âøó
########################################

# ‰ªªÂä°ÂáΩÊï∞ÔºöÊâßË°åÊã∑Ë¥ù
do_copy() {
    local today_str=$(date +%F)          # 2025-08-05
    local dest_dir="${DEST_BASE}/${today_str}"
    mkdir -p "$dest_dir"

    # Êü•ÊâæÂπ∂Â§çÂà∂
    for f in "$SRC_DIR"/*"$today_str"*; do
        [ -e "$f" ] || continue          # Êó†ÂåπÈÖçÊó∂Ë∑≥Ëøá
        cp -n "$f" "$dest_dir/"
    done

    echo "[$(date '+%F %T')] Â∑≤Êã∑Ë¥ùÂà∞ $dest_dir" >> "$LOG_FILE"
}

# Êó†ÈôêÂæ™ÁéØ
while true; do
    now=$(date +%H:%M)
    case "$now" in
        11:00|13:00|15:00|16:00)
            do_copy
            # Èò≤Ê≠¢Âêå‰∏ÄÂàÜÈíüÂÜÖÈáçÂ§çËß¶Âèë
            sleep 70
            ;;
    esac
    # ÊØè 30 ÁßíÊ£ÄÊü•‰∏ÄÊ¨°Êó∂Èó¥
    sleep 30
done









